---
title: "COMPSCIX 415.2 Homework 9/Final"
author: "ben kimel green"
date: "3/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
```
##Exercise 1

###STEP 1

Write an R function that does the following:

* Takes a sample of size samp_size from this exponential distribution (samp_size is an input parameter for the function)
* Calculates the mean of that sample
* Calculates the standard deviation of that sample
* Returns the calculated mean and standard deviation as a list
```{r}
samp_fun <- function(samp_size, samp_rate) {
  
  sample <- rexp(n = samp_size, rate = samp_rate)
  samp_avg <- mean(sample)
  samp_std_dev <-sd(sample)
  stats <- list(samp_avg = samp_avg, samp_std_dev = samp_std_dev)
  return(stats) 
  }
```

###STEP 2

write a loop that does this:

* Runs the above function 1000 times, with samp_size = 50 and samp_rate = 1/10000
* Saves all of the sample means in a vector called sample_means, and all of the sample standard deviations in a vector called `sample_sds`
```{r}
n_loops <-1000
samp_size <- 50
samp_rate <- 1/10000
sample_means <- rep(NA, n_loops)
sample_sds <- rep(NA, n_loops)
for (i in 1:n_loops) {
  resVector <- samp_fun(samp_size,samp_rate)
  sample_means[[i]]<- resVector$samp_avg
  sample_sds[[i]]<-resVector$samp_std_dev
}
```

### STEP 3

* plot your sample means as a histogram
* output the standard deviation of your sample means
* calculate the theoretical standard error (σ = 10000, n = sample size)
* calculate the mean of the sample standard deviations and use this to calculate the empirical standard
error
```{r}
tibble_means <- as.tibble(sample_means)
ggplot(tibble_means,aes(x = value))+geom_histogram(bins = 15)
sd(sample_means)
10000/sqrt(samp_size)
```
### STEP 4
Repeat STEP 2 and STEP 3 using a sample size of 5000.
```{r}
samp_size_big = 5000
for (i in 1:n_loops) {
  resVector <- samp_fun(samp_size_big,samp_rate)
  sample_means[[i]]<- resVector$samp_avg
  sample_sds[[i]]<-resVector$samp_std_dev
}
tibble_means <- enframe(sample_means)
ggplot(tibble_means,aes(x = value))+geom_histogram(bins = 15)
sd(sample_means)
10000/sqrt(samp_size_big)
```

##Exercise 2
For this exercise we will return to the House Prices prediction dataset that we used for HW 7. You should have already downloaded the train.csv dataset before, but if you didn’t you can download it from Canvas in this week’s module.
Load the train.csv dataset into R and fit a regression model with:
* y = SalePrice
* Features: LotArea, OverallQual, and ExterQual
```{r}
housesData <- read_csv("train.csv")
model <- lm(formula = SalePrice ~ LotArea + OverallQual+ ExterQual, data = housesData)
```
Answer these questions:
* Use the broom package to output the coefficients and the R-squared
```{r}
glance(model)
```

* Interpret the coefficient on LotArea
A:<br/> 
The coefficient is 1.4552925. thia meeans that for every square fit increase in lot size the house price will cost 1.45$ more. 
```{r}
tidy(model)
```

* Interpret the coefficient on ExterQualGd
A:<br/> 
The `ExterQual` data point describes the quality of exterior finish of a house. this model takes `Excellent` Quality and compares it to the  other values avilable for this data point.<br/> `ExterQualGood` (good finish quality) house will cost on avarage 71,529$ less then a house with excellent exterior finish.<br/>
* Compare this model to the model we fit in HW 7 with GrLivArea, OverallQual, Neighborhood. Which
is the better fitting model?
A: The p value on both is 0 so they are statistically segnificant. The r squared is much higher on the HW7 model and so is the adjusted r squared so in that sense it is better. <br/>
I also think the HW7 model makes more sense. The current one uses 2 quality index at the same time, they are not complitly related but it's a close enough to have second thoughts about the model. <br/>
HW7 model compared neighborhood to quality of finish to living space- knowing what a house is it seems to make more sense to use these data points over the ones used here.
Another reason why the current model is problematic is demonstrated in the plot below. Most of the samples are `good` or `typical` this makes this feature weak.
```{r}
ggplot(housesData,aes(x = ExterQual))+geom_bar()
```

## Exercise 3

Download the ab_test_data.csv file from Canvas. This file contains two columns: version and conversion. Each row is a visitor to a webpage. The version column tells us which version of the webpage the visitor saw, and the conversion column is a binary value and equals 1 if the visitor converted (0 otherwise).
```{r}
ABData <- read_csv("ab_test_data.csv")
```

We want to perform an AB test on this data to see if the conversion rates are different for the two versions of the webpage.
Answer these questions:
a. What proportion of visitors converted for each version of the webpage?

Answer: the output below shows that sit a had 4.15% conversion and site B has 10% conversion

```{r}
a_conversions<-filter(ABData,version == "A")
mean(a_conversions$conversion)

b_conversions<-filter(ABData,version == "B")
mean(b_conversions$conversion)

```

b. Perform the AB test in R. What is the p-value for the AB test (hypothesis test of proportions)?
Answer: the output below shows the diffrence in conversion rate to be segnificant. 
```{r}

two_prop_test <- prop.test(c(sum(a_conversions$conversion), sum(b_conversions$conversion)), c(2000, 2000))
two_prop_test$p.value

```

